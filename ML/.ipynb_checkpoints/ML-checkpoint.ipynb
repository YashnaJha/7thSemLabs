{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Github Link](https://github.com/rednithin/7thSemLabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sunny', 'Warm', '?', 'Strong', '?', '?']\n"
     ]
    }
   ],
   "source": [
    "from csv import reader\n",
    "from pprint import pprint\n",
    "\n",
    "with open(\"1-dataset.csv\") as f:\n",
    "    dataset = [row[:-1] for row in reader(f) if row[-1] == \"Yes\"]\n",
    "\n",
    "hypothesis = dataset[0][:]\n",
    "for row in dataset[1:]:\n",
    "    hypothesis = [\"?\" if tup[0] != tup[1] else tup[0]\n",
    "                  for tup in zip(hypothesis, row)]\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sunny', 'Warm', '?', 'Strong', '?', '?']\n",
      "[['Sunny', '?', '?', '?', '?', '?'], ['?', 'Warm', '?', '?', '?', '?']]\n"
     ]
    }
   ],
   "source": [
    "from csv import reader\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def classify(hypo, row):\n",
    "    for a, b in zip(hypo, row):\n",
    "        if not(a == '?' or a == b):\n",
    "            return 'No'\n",
    "    return 'Yes'\n",
    "\n",
    "\n",
    "with open(\"1-dataset.csv\") as f:\n",
    "    dataset = [row for row in reader(f)]\n",
    "\n",
    "positive_dataset = [row[:-1] for row in dataset if row[-1] == \"Yes\"]\n",
    "negative_dataset = [row[:-1] for row in dataset if row[-1] == \"No\"]\n",
    "hypo_len = len(positive_dataset[0])\n",
    "specific_hypothesis = positive_dataset[0][:]\n",
    "generic_hypothesis = [['?'] * hypo_len]\n",
    "\n",
    "for row in positive_dataset[1:]:\n",
    "    specific_hypothesis = [\"?\" if tup[0] != tup[1] else tup[0]\n",
    "                           for tup in zip(specific_hypothesis, row)]\n",
    "\n",
    "for row in negative_dataset:\n",
    "    newHypothesis = []\n",
    "    for hypo in generic_hypothesis:\n",
    "        if classify(hypo, row) == 'Yes':\n",
    "            candidates = [hypo[:] for _ in range(hypo_len)]\n",
    "            for i in range(hypo_len):\n",
    "                if candidates[i][i] == '?':\n",
    "                    candidates[i][i] = specific_hypothesis[i]\n",
    "            newHypothesis += candidates\n",
    "    generic_hypothesis += newHypothesis\n",
    "    generic_hypothesis = list(\n",
    "        filter(lambda x: True if classify(x, row) == 'No' else False, generic_hypothesis))\n",
    "\n",
    "print(specific_hypothesis)\n",
    "print(generic_hypothesis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Outlook=Overcast => Yes',\n",
      " 'Outlook=Rain ^ Wind=F => Yes',\n",
      " 'Outlook=Rain ^ Wind=T => No',\n",
      " 'Outlook=Sunny ^ Humidity=High => No',\n",
      " 'Outlook=Sunny ^ Humidity=Normal => Yes']\n",
      "Outlook : Overcast\n",
      "Temperature : \n",
      "Humidity : \n",
      "Wind : \n",
      "'Yes'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import log2\n",
    "from csv import reader\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "\n",
    "YES = 'Y'\n",
    "NO = 'N'\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, label):\n",
    "        self.label = label\n",
    "        self.branches = {}\n",
    "\n",
    "\n",
    "def entropy(data):\n",
    "    total, posititve, negative = len(\n",
    "        data), (data[:, -1] == YES).sum(), (data[:, -1] == NO).sum()\n",
    "    entropy = 0\n",
    "    if posititve > 0:\n",
    "        entropy -= posititve / total * log2(posititve / total)\n",
    "    if negative > 0:\n",
    "        entropy -= negative / total * log2(negative / total)\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def gain(s, data, column):\n",
    "    values = set(data[:, column])\n",
    "    gain = s\n",
    "    for value in values:\n",
    "        sub = data[data[:, column] == value]\n",
    "        gain -= len(sub) / len(data) * entropy(sub)\n",
    "    return gain\n",
    "\n",
    "\n",
    "def bestAttribute(data):\n",
    "    s = entropy(data)\n",
    "    maxCol = -1\n",
    "    maxGain = -float('inf')\n",
    "\n",
    "    for column in range(len(data[0]) - 1):\n",
    "        g = gain(s, data, column)\n",
    "        if g > maxGain:\n",
    "            maxGain = g\n",
    "            maxCol = column\n",
    "\n",
    "    return maxCol\n",
    "\n",
    "\n",
    "def id3(data, labels):\n",
    "    root = Node(\"NULL\")\n",
    "\n",
    "    if (entropy(data) == 0):\n",
    "        if data[0, -1] == YES:\n",
    "            root.label = \"Yes\"\n",
    "        else:\n",
    "            root.label = \"No\"\n",
    "    elif len(data[0]) == 1:\n",
    "        root.label = Counter(data[:, -1]).most_common(n=1)[0]\n",
    "    else:\n",
    "        column = bestAttribute(data)\n",
    "        root.label = labels[column]\n",
    "        values = set(data[:, column])\n",
    "\n",
    "        for value in values:\n",
    "            nData = np.delete(data[data[:, column] == value], column, axis=1)\n",
    "            nLabels = np.delete(labels, column)\n",
    "            root.branches[value] = id3(nData, nLabels)\n",
    "    return root\n",
    "\n",
    "\n",
    "def getRules(root, rule, rules):\n",
    "\n",
    "    if not root.branches:\n",
    "        rules.append(rule[:-2] + '=> ' + root.label)\n",
    "    for i in root.branches:\n",
    "        getRules(root.branches[i], rule + root.label + '=' + i + \" ^ \", rules)\n",
    "\n",
    "\n",
    "def predict(tree, tup):\n",
    "    if not tree.branches:\n",
    "        return tree.label\n",
    "\n",
    "    return predict(tree.branches[tup[tree.label]], tup)\n",
    "\n",
    "\n",
    "labels = np.array(['Outlook', 'Temperature', 'Humidity', 'Wind', 'PlayTennis'])\n",
    "\n",
    "with open('3-dataset.csv') as f:\n",
    "    data = np.array(list(reader(f)))\n",
    "\n",
    "\n",
    "tree = id3(data, labels)\n",
    "rules = []\n",
    "getRules(tree, \"\", rules)\n",
    "pprint(sorted(rules))\n",
    "\n",
    "tup = {}\n",
    "for label in labels[:-1]:\n",
    "    tup[label] = input(label + \" : \")\n",
    "\n",
    "pprint(predict(tree, tup))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Error: 4.947865303680527\n",
      "Epoch: 100 Error: 0.794197198281876\n",
      "Epoch: 200 Error: 0.4993039570506267\n",
      "Epoch: 300 Error: 0.33155708656192456\n",
      "Final Weights W1 and W2\n",
      "array([[ 1.91612562, -2.56298013],\n",
      "       [ 2.66954077,  0.08924304],\n",
      "       [-0.68921909,  1.45600405]])\n",
      "array([[ 6.29558109, -1.526356  , -1.12090469]])\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(5)\n",
    "\n",
    "\n",
    "def nonlin(x, deriv=False):\n",
    "    if deriv == True:\n",
    "        return x * (1 - x)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "n_x, n_h, n_y, lRate = 2, 3, 1, 0.5\n",
    "\n",
    "W1 = np.random.randn(n_h, n_x)\n",
    "b1 = np.random.randn(n_h, 1)\n",
    "W2 = np.random.randn(n_y, n_h)\n",
    "b2 = np.random.randn(n_y, 1)\n",
    "\n",
    "dataset = [[2.7810836, 2.550537003, 0],\n",
    "           [1.465489372, 2.362125076, 0],\n",
    "           [3.396561688, 4.400293529, 0],\n",
    "           [1.38807019, 1.850220317, 0],\n",
    "           [3.06407232, 3.005305973, 0],\n",
    "           [7.627531214, 2.759262235, 1],\n",
    "           [5.332441248, 2.088626775, 1],\n",
    "           [6.922596716, 1.77106367, 1],\n",
    "           [8.675418651, -0.242068655, 1],\n",
    "           [7.673756466, 3.508563011, 1]]\n",
    "\n",
    "for epoch in range(300 + 1):\n",
    "    total_error = 0\n",
    "    for example in dataset:\n",
    "        attrs = np.array(example[:-1]).reshape(-1, 1)\n",
    "        target = example[-1]\n",
    "\n",
    "        l0 = attrs\n",
    "        l1 = nonlin(W1 @ l0 + b1)\n",
    "        l2 = nonlin(W2 @ l1 + b2)\n",
    "\n",
    "        l2_error = target - l2\n",
    "        l2_delta = l2_error * nonlin(l2, deriv=True)\n",
    "\n",
    "        l1_error = W2.T @ l2_delta\n",
    "        l1_delta = l1_error * nonlin(l1, deriv=True)\n",
    "\n",
    "        W2 += lRate * l2_delta @ l1.T\n",
    "        b2 += lRate * l2_delta\n",
    "        W1 += lRate * l1_delta @ l0.T\n",
    "        b1 += lRate * l1_delta\n",
    "\n",
    "        total_error += np.absolute(l2_error).sum()\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch: {epoch} Error: {total_error}')\n",
    "\n",
    "print('Final Weights W1 and W2')\n",
    "pprint(W1)\n",
    "pprint(W2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: 10.0 \tPredicted: 10.0\n",
      "Actual: 5.0 \tPredicted: 5.0\n",
      "Actual: 10.0 \tPredicted: 5.0\n",
      "Actual: 10.0 \tPredicted: 10.0\n",
      "Actual: 10.0 \tPredicted: 10.0\n",
      "Actual: 0.0 \tPredicted: 0.0\n",
      "Actual: 0.0 \tPredicted: 0.0\n",
      "Actual: 5.0 \tPredicted: 5.0\n",
      "Actual: 5.0 \tPredicted: 5.0\n",
      "Actual: 5.0 \tPredicted: 5.0\n",
      "Actual: 0.0 \tPredicted: 0.0\n",
      "Actual: 5.0 \tPredicted: 5.0\n",
      "Actual: 0.0 \tPredicted: 0.0\n",
      "Actual: 0.0 \tPredicted: 0.0\n",
      "Actual: 0.0 \tPredicted: 0.0\n",
      "Accuracy 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "from csv import reader\n",
    "from pprint import pprint\n",
    "from random import shuffle, seed\n",
    "from math import exp, pi, sqrt\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "\n",
    "seed(2)\n",
    "data = np.array(list(reader(open('5-dataset-alt.csv'))), dtype='float')\n",
    "shuffle(data)\n",
    "trainLen = int(.9 * len(data))\n",
    "trainData, trainTarget = data[:trainLen, : -1], data[:trainLen, -1]\n",
    "testData, testTarget = data[trainLen:, : -1], data[trainLen:, -1]\n",
    "\n",
    "\n",
    "def safe_div(x, y):\n",
    "    return x / y if y != 0 else 0\n",
    "\n",
    "\n",
    "def getProbabilty(x, mean, std):\n",
    "    exponent = exp(-safe_div((x - mean) ** 2, 2 * std ** 2))\n",
    "    return safe_div(1, sqrt(2 * pi) * std) * exponent\n",
    "\n",
    "\n",
    "classes = {}\n",
    "for attrs, target in zip(trainData, trainTarget):\n",
    "    if target not in classes:\n",
    "        classes[target] = []\n",
    "    classes[target].append(attrs)\n",
    "\n",
    "summaries = {}\n",
    "for cls in classes.keys():\n",
    "    summaries[cls] = []\n",
    "    for column in zip(*classes[cls]):\n",
    "        summaries[cls].append((np.mean(column), np.std(column)))\n",
    "\n",
    "correct = 0\n",
    "for attrs, target in zip(testData, testTarget):\n",
    "    probabilty = {}\n",
    "    for cls in classes.keys():\n",
    "        probabilty[cls] = 1\n",
    "        for i, (mean, std) in enumerate(summaries[cls]):\n",
    "            probabilty[cls] *= getProbabilty(attrs[i], mean, std)\n",
    "\n",
    "    cls = sorted(probabilty.items(), key=itemgetter(1), reverse=True)[0][0]\n",
    "    print(f'Actual: {target} \\tPredicted: {cls}')\n",
    "    if cls == target:\n",
    "        correct += 1\n",
    "\n",
    "print(f'Accuracy {correct/len(testData)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : \n",
      "0.8348868175765646\n",
      "Classification Report : \n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.97      0.60      0.74       319\n",
      "         comp.graphics       0.96      0.89      0.92       389\n",
      "               sci.med       0.97      0.81      0.88       396\n",
      "soc.religion.christian       0.65      0.99      0.78       398\n",
      "\n",
      "           avg / total       0.88      0.83      0.84      1502\n",
      "\n",
      "Confusion Matrix : \n",
      "[[192   2   6 119]\n",
      " [  2 347   4  36]\n",
      " [  2  11 322  61]\n",
      " [  2   2   1 393]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "categories = ['alt.atheism', 'soc.religion.christian',\n",
    "              'comp.graphics', 'sci.med']\n",
    "\n",
    "train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "\n",
    "c, t = CountVectorizer(), TfidfTransformer()\n",
    "\n",
    "trainTr = t.fit_transform(c.fit_transform(train.data))\n",
    "testTr = t.transform(c.transform(test.data))\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(trainTr, train.target)\n",
    "predicted = model.predict(testTr)\n",
    "\n",
    "print('Accuracy : ', accuracy_score(test.target, predicted), sep='\\n')\n",
    "print('Classification Report : ', classification_report(\n",
    "    test.target, predicted, target_names=test.target_names), sep='\\n')\n",
    "print('Confusion Matrix : ', confusion_matrix(test.target, predicted), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.7/site-packages/bayespy/inference/vmp/nodes/categorical.py:107: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  u0[[np.arange(np.size(x)), np.ravel(x)]] = 1\n"
     ]
    }
   ],
   "source": [
    "from bayespy.nodes import Dirichlet, Categorical, MultiMixture\n",
    "import numpy as np\n",
    "from csv import reader\n",
    "\n",
    "enum = [{'SSC': 0, 'S': 1, 'M': 2, 'Y': 3, 'T': 4}, {'M': 0, 'F': 1}, {'Y': 0, 'N': 1}, {'H': 0, 'M': 1, 'L': 2}, {\n",
    "        'Athlete': 0, 'Active': 1, 'Moderate': 2, 'Sedetary': 3}, {'H': 0, 'B': 1, 'N': 2}, {'Y': 0, 'N': 1}]\n",
    "\n",
    "data = np.array([[enum[i][j]\n",
    "                  for i, j in enumerate(k)] for k in reader(open('7-dataset.csv'))])\n",
    "\n",
    "n = len(data)\n",
    "\n",
    "categoricals = []\n",
    "for i in range(len(enum)-1):\n",
    "    dirichlet = Dirichlet(np.ones(len(enum[i])))\n",
    "    categoricals.append(Categorical(dirichlet, plates=(n,)))\n",
    "    categoricals[i].observe(data[:, i])\n",
    "\n",
    "target = Dirichlet(np.ones(2), plates=(5, 2, 2, 3, 4, 3))\n",
    "model = MultiMixture(categoricals, Categorical, target)\n",
    "model.observe(data[:, -1])\n",
    "target.update()\n",
    "\n",
    "tup = [enum[i][j] for i, j in enumerate(input('Tuple: ').split(','))]\n",
    "\n",
    "result = MultiMixture(tup, Categorical, target).get_moments()[0][0]\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colormap = np.array(['red', 'lime', 'black'])\n",
    "iris = load_iris()\n",
    "X, Y = iris.data, iris.target\n",
    "\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(X)\n",
    "\n",
    "gmm = GaussianMixture(n_components=3)\n",
    "gmm.fit(X)\n",
    "gmmY = gmm.predict(X)\n",
    "\n",
    "print('Kmeans', silhouette_score(X, kmeans.labels_))\n",
    "print('GMM', silhouette_score(X, gmmY))\n",
    "\n",
    "\n",
    "def plot(i, title, targets):\n",
    "    plt.subplot(2, 2, i)\n",
    "    plt.scatter(X[:, 2], X[:, 3], c=colormap[targets])\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Petal Length')\n",
    "    plt.ylabel('Petal Width')\n",
    "\n",
    "\n",
    "plot(1, 'Real', Y)\n",
    "plot(2, 'K Means', kmeans.labels_)\n",
    "plot(3, 'Real', Y)\n",
    "plot(4, 'GMM', gmmY)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import shuffle, seed\n",
    "from csv import reader\n",
    "from collections import Counter\n",
    "\n",
    "seed(4)\n",
    "\n",
    "with open('9-dataset.csv') as f:\n",
    "    dataset = np.array(list(reader(f)))\n",
    "    shuffle(dataset)\n",
    "\n",
    "trainLen = int(0.8 * len(dataset))\n",
    "trainDataset, trainTarget = np.array(\n",
    "    dataset[:trainLen, :-1], dtype='float'), dataset[:trainLen, -1]\n",
    "testDataset, testTarget = np.array(\n",
    "    dataset[trainLen:, :-1], dtype='float'), dataset[trainLen:, -1]\n",
    "\n",
    "predicted = []\n",
    "k = 5\n",
    "for row in testDataset:\n",
    "    eds = list(map(lambda x: np.sum((row - x) ** 2), trainDataset))\n",
    "    kNearest = np.array(sorted(zip(eds, trainTarget))[:k])[:, -1]\n",
    "    labelFreq = Counter(kNearest)\n",
    "    predicted.append(labelFreq.most_common(n=1)[0][0])\n",
    "\n",
    "correct = 0\n",
    "for i, target in enumerate(testTarget):\n",
    "    print(f'Actual: {target.ljust(15)} \\t\\tPredicted: {predicted[i]}')\n",
    "    if predicted[i] == target:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"Accuracy : {correct/len(testTarget)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program 10 + Alternate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import pylab as pl\n",
    "from math import pi, ceil\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "\n",
    "def lowess(x, y, f=2. / 3., iter=3):\n",
    "    n = len(x)\n",
    "    r = int(ceil(f * n))\n",
    "    h = [np.sort(np.abs(x - x[i]))[r] for i in range(n)]\n",
    "\n",
    "    w = np.clip(np.abs((x[:, None] - x[None, :]) / h), 0.0, 1.0)\n",
    "    w = (1 - w ** 3) ** 3\n",
    "\n",
    "    yest = np.zeros(n)\n",
    "    delta = np.ones(n)\n",
    "    for _ in range(iter):\n",
    "        for i in range(n):\n",
    "            weights = delta * w[:, i]\n",
    "            b = np.array([np.sum(weights * y), np.sum(weights * y * x)])\n",
    "            A = np.array([[np.sum(weights), np.sum(weights * x)],\n",
    "                          [np.sum(weights * x), np.sum(weights * x * x)]])\n",
    "            beta = np.linalg.solve(A, b)\n",
    "            yest[i] = beta[0] + beta[1] * x[i]\n",
    "\n",
    "        residuals = y - yest\n",
    "        s = np.median(np.abs(residuals))\n",
    "        delta = np.clip(residuals / (6.0 * s), -1, 1)\n",
    "        delta = (1 - delta ** 2) ** 2\n",
    "\n",
    "    return yest\n",
    "\n",
    "\n",
    "n = 100\n",
    "x = np.linspace(0, 2 * pi, n)\n",
    "y = np.sin(x) + 0.3 * np.random.randn(n)\n",
    "\n",
    "f = 0.25\n",
    "yest = lowess(x, y, f=f, iter=3)\n",
    "\n",
    "'''-----------------------------------------'''\n",
    "n = 200\n",
    "x = np.linspace(-pi, 3 * pi, n)\n",
    "y = np.sin(x) + 0.3 * np.random.randn(n)\n",
    "\n",
    "neigh = KNeighborsRegressor(n_neighbors=30)\n",
    "neigh.fit(x.reshape(-1, 1), y.reshape(-1, 1))\n",
    "newY = neigh.predict(x.reshape(-1, 1))[50:150]\n",
    "x, y = x[50:150], y[50:150]\n",
    "\n",
    "pl.clf()\n",
    "pl.plot(x, y, label='Noisy')\n",
    "pl.plot(x, yest, label='Lowess')\n",
    "pl.plot(x, newY, label='KNN Scikit')\n",
    "pl.legend(loc='upper right')\n",
    "pl.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
